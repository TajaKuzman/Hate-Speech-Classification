{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hate Speech Binary Classification","metadata":{}},{"cell_type":"markdown","source":"Import all necessary libraries and install everything you need for training:","metadata":{}},{"cell_type":"markdown","source":"First, enable the GPU - under Accelerator on the right of the site, choose GPU. Be careful to always terminate the session (click the power off button), otherwise it will still be running and you will lose the 30 hours of GPU that you have available per week.","metadata":{}},{"cell_type":"code","source":"# install the libraries necessary for data wrangling, prediction and result analysis\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nimport torch\nfrom numba import cuda\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:01.148583Z","iopub.execute_input":"2022-06-30T12:25:01.149275Z","iopub.status.idle":"2022-06-30T12:25:05.636300Z","shell.execute_reply.started":"2022-06-30T12:25:01.149170Z","shell.execute_reply":"2022-06-30T12:25:05.632600Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Install transformers\n# (this needs to be done on Kaggle each time you start the session)\n!pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:05.641796Z","iopub.execute_input":"2022-06-30T12:25:05.642376Z","iopub.status.idle":"2022-06-30T12:25:16.382645Z","shell.execute_reply.started":"2022-06-30T12:25:05.642338Z","shell.execute_reply":"2022-06-30T12:25:16.381553Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Install the simpletransformers\n!pip install -q simpletransformers\nfrom simpletransformers.classification import ClassificationModel","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:16.384553Z","iopub.execute_input":"2022-06-30T12:25:16.384910Z","iopub.status.idle":"2022-06-30T12:25:44.937840Z","shell.execute_reply.started":"2022-06-30T12:25:16.384876Z","shell.execute_reply":"2022-06-30T12:25:44.936739Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Import the data\nYou might need to upload the data (click on the Add data button on the left of the site). I have uploaded the first version of the data that I created (see the 1-Data-Preparation.ipynb spreadsheet): \"hatespeechdataset\". If you change the Google Sheet, you can reprocess it by running the data preparation spreadsheet and upload the new version of it (go to the dataset description (https://www.kaggle.com/datasets/tajakuz/hatespeechdataset), click on the three dots and choose \"New Version)","metadata":{}},{"cell_type":"code","source":"# Upload the binary hate speech dataset\nhs_dataset = pd.read_csv(\"/kaggle/input/hatespeechdataset/hatespeech_binary_dataset.csv\", sep=\"\\t\")\nhs_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:44.939768Z","iopub.execute_input":"2022-06-30T12:25:44.940972Z","iopub.status.idle":"2022-06-30T12:25:45.011236Z","shell.execute_reply.started":"2022-06-30T12:25:44.940918Z","shell.execute_reply":"2022-06-30T12:25:45.010008Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# See the statistics on the dataset\nhs_dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.015264Z","iopub.execute_input":"2022-06-30T12:25:45.015534Z","iopub.status.idle":"2022-06-30T12:25:45.040026Z","shell.execute_reply.started":"2022-06-30T12:25:45.015509Z","shell.execute_reply":"2022-06-30T12:25:45.038789Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Define the labels\nLABELS = [0,1]","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.041631Z","iopub.execute_input":"2022-06-30T12:25:45.042321Z","iopub.status.idle":"2022-06-30T12:25:45.046800Z","shell.execute_reply.started":"2022-06-30T12:25:45.042284Z","shell.execute_reply":"2022-06-30T12:25:45.045650Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# First, let's split the dataset into train and test split (70:30) by using the train_test_split from the sci-kit learn. We will shuffle the data beforehand and stratify it according to the labels (so that the distribution of labels is the same in both splits as in the original dataset)\n\nhs_train, hs_test = train_test_split(hs_dataset, test_size=0.3, random_state=42, shuffle = True, stratify = hs_dataset.labels)\n\n# See the size of the splits\nhs_train.shape, hs_test.shape\n","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.048390Z","iopub.execute_input":"2022-06-30T12:25:45.049203Z","iopub.status.idle":"2022-06-30T12:25:45.066256Z","shell.execute_reply.started":"2022-06-30T12:25:45.049167Z","shell.execute_reply":"2022-06-30T12:25:45.065289Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Check how the splits look like\nhs_train.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.067606Z","iopub.execute_input":"2022-06-30T12:25:45.067932Z","iopub.status.idle":"2022-06-30T12:25:45.077330Z","shell.execute_reply.started":"2022-06-30T12:25:45.067899Z","shell.execute_reply":"2022-06-30T12:25:45.076370Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Check how the splits look like\nhs_test.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.078663Z","iopub.execute_input":"2022-06-30T12:25:45.079451Z","iopub.status.idle":"2022-06-30T12:25:45.091850Z","shell.execute_reply.started":"2022-06-30T12:25:45.079404Z","shell.execute_reply":"2022-06-30T12:25:45.090832Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Create a file to save results into (you can find it under Data: Output). Be careful, run this step only once to not overwrite the results file.\nresults = []\n\nwith open(\"HateSpeech-Experiments-Results.json\", \"w\") as results_file:\n    json.dump(results,results_file, indent= \"\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.093669Z","iopub.execute_input":"2022-06-30T12:25:45.094306Z","iopub.status.idle":"2022-06-30T12:25:45.100090Z","shell.execute_reply.started":"2022-06-30T12:25:45.094270Z","shell.execute_reply":"2022-06-30T12:25:45.098869Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# In each next step (after the first experiment), open the results file instead of creating a new results file:\nwith open(\"./HateSpeech-Experiments-Results.json\", \"r\") as results_file:\n    previous_results = json.load(results_file)\n\n# See the results\nprevious_results","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.101745Z","iopub.execute_input":"2022-06-30T12:25:45.102841Z","iopub.status.idle":"2022-06-30T12:25:45.112571Z","shell.execute_reply.started":"2022-06-30T12:25:45.102801Z","shell.execute_reply":"2022-06-30T12:25:45.111508Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training and testing - dummy classifier\n\nLet's first apply a baseline classifier which predicts the most frequent class to each instance, to see what is the baseline score.","metadata":{"execution":{"iopub.execute_input":"2022-06-29T12:11:51.221801Z","iopub.status.busy":"2022-06-29T12:11:51.221298Z","iopub.status.idle":"2022-06-29T12:11:51.233411Z","shell.execute_reply":"2022-06-29T12:11:51.231818Z","shell.execute_reply.started":"2022-06-29T12:11:51.22176Z"}}},{"cell_type":"code","source":"# Create X_train and Y_train parts, used for sci kit learning\n# We need to split each split (test and train) into an object with just texts and object with just labels\nX_train = list(hs_train.text)\nY_train = list(hs_train.labels)\n\nX_test = list(hs_test.text)\nY_test = list(hs_test.labels)\n\n# See their sizes\nlen(X_train), len(Y_train), len(X_test), len(Y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.114352Z","iopub.execute_input":"2022-06-30T12:25:45.115092Z","iopub.status.idle":"2022-06-30T12:25:45.130639Z","shell.execute_reply.started":"2022-06-30T12:25:45.115002Z","shell.execute_reply":"2022-06-30T12:25:45.129495Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Use the Dummy Classifier, with the strategy \"most_frequent\"\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\n\n# Train the model\ndummy_clf.fit(X_train, Y_train)\n\n#Get the predictions\ny_pred = dummy_clf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.132251Z","iopub.execute_input":"2022-06-30T12:25:45.132638Z","iopub.status.idle":"2022-06-30T12:25:45.141123Z","shell.execute_reply.started":"2022-06-30T12:25:45.132600Z","shell.execute_reply":"2022-06-30T12:25:45.140171Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Compare the predictions with true values (Y_test)\nmicro = f1_score(Y_test, y_pred, labels=LABELS, average =\"micro\")\nmacro = f1_score(Y_test, y_pred, labels=LABELS, average =\"macro\")\naccuracy = round(metrics.accuracy_score(Y_test, y_pred),3)\nprint(f\"Micro F1: {micro:.3f}, Macro F1: {macro:.3f}, Accuracy: {accuracy}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.145789Z","iopub.execute_input":"2022-06-30T12:25:45.146423Z","iopub.status.idle":"2022-06-30T12:25:45.161789Z","shell.execute_reply.started":"2022-06-30T12:25:45.146382Z","shell.execute_reply":"2022-06-30T12:25:45.160648Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Save the results:\nrezdict = {\n    \"model\": \"dummy\",\n    \"microF1\": micro,\n    \"macroF1\": macro,\n    \"accuracy\": accuracy,\n    \"y_true\": Y_test.tolist(),\n    \"y_pred\": {y_pred.tolist()}\n    }\nprevious_results.append(rezdict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"previous_results","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.171814Z","iopub.execute_input":"2022-06-30T12:25:45.172727Z","iopub.status.idle":"2022-06-30T12:25:45.182723Z","shell.execute_reply.started":"2022-06-30T12:25:45.172673Z","shell.execute_reply":"2022-06-30T12:25:45.181385Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Training and testing - Transformer model","metadata":{}},{"cell_type":"markdown","source":"We will use the basic English monolingual BERT model: https://huggingface.co/bert-base-uncased","metadata":{}},{"cell_type":"markdown","source":"You can find more documentation on how to use Simple Transformer models here: https://simpletransformers.ai/docs/usage/\n\nFor the hyperparameters (args), I used the ones that worked for me before, but you can see the entire list here: https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model","metadata":{}},{"cell_type":"code","source":"# Define the model\nbertbase_model = ClassificationModel(\n        \"bert\", \"bert-base-cased\",\n        num_labels=2,\n        use_cuda=True,\n        args= {\n    # Here, we have much more instances than in Experiment 1, so we can use less epochs.\n    \"num_train_epochs\": 20,\n    \"labels_list\": LABELS,\n    \"learning_rate\": 1e-5,\n    # We'll use a smaller max_seq_length (we could set it up to 512), because we have short texts\n    \"max_seq_length\": 128,\n    # Use this to mute the long output that tells you how the model proceeds.\n    \"silent\": True,\n    # Below are just some additional hyperparameters that we found that help with memory errors\n    \"save_steps\": -1,\n    \"overwrite_output_dir\": True,\n    \"no_cache\": True,\n    \"no_save\": True,\n    }\n    )","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:45.184141Z","iopub.execute_input":"2022-06-30T12:25:45.185022Z","iopub.status.idle":"2022-06-30T12:25:58.840975Z","shell.execute_reply.started":"2022-06-30T12:25:45.184982Z","shell.execute_reply":"2022-06-30T12:25:58.840003Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Train the model on train data - this will take some time\nbertbase_model.train_model(hs_train)\n\nprint(\"Training is finished!\")","metadata":{"execution":{"iopub.status.busy":"2022-06-30T12:25:58.842455Z","iopub.execute_input":"2022-06-30T12:25:58.842792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the model - this will take some time\n\n# Get the true labels\ny_true = hs_test.labels\n\n# Calculate the model's predictions on test\ndef make_prediction(input_string):\n    return bertbase_model.predict([input_string])[0][0]\n\ny_pred = hs_test.text.apply(make_prediction)\n\nprint(\"Testing is finished!\")","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate the scores\nmacro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\nmicro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\naccuracy = round(metrics.accuracy_score(y_true, y_pred),3)\nprint(f\"Macro f1: {macro:0.3}, Micro f1: {micro:0.3}, Accuracy: {accuracy}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the results:\nrezdict = {\n    \"model\": \"BERT\",\n    \"epoch\": 30,\n    \"microF1\": micro,\n    \"macroF1\": macro,\n    \"accuracy\": accuracy,\n    \"y_true\": Y_test.tolist(),\n    \"y_pred\": y_pred.tolist(),\n    }\nprevious_results.append(rezdict)\n\n#Save intermediate results (just in case)\nbackup = []\nbackup.append(rezdict)\nwith open(f\"backup-results.json\", \"w\") as backup_file:\n    json.dump(backup,backup_file, indent= \"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compare the results by creating a dataframe from the previous_results dictionary:\nresults_df = pd.DataFrame(previous_results)\n\nresults_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(results_df.drop(columns=[\"y_pred\",\"epoch\"]).to_markdown())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that BERT performs better than the baseline and that we get the best results when we train the model for 20 epochs.","metadata":{}},{"cell_type":"code","source":"# Add the end, save the file with results:\nwith open(\"./HateSpeech-Experiments-Results.json\", \"w\") as final_results_file:\n    json.dump(previous_results,final_results_file, indent= \"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}